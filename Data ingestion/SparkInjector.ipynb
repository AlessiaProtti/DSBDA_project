{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Spark injector\n",
    "\n",
    "The purpose of this notebook is to build the injection mechanism using SparkStreaming.\n",
    "Data is read through the network in batches, meaning each RDD will contain multiple rows. Our job then is to convert each RDD in a spark dataframe and save it to HDFS in CSV format."
   ],
   "id": "1ce8f77bf5859acc"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "id": "69f24994877007ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession"
   ],
   "id": "efda47df15c72903",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conversion function\n",
    "\n",
    "Here we define the function responsible for the CSV conversion. It is fed an RDD whose content is a batch of rows sent through the network.\n",
    "The function will then take each row, convert it to a Spark DataFrame and save it to HDFS."
   ],
   "id": "cb8f13e5db93566b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rdd_to_hdfs(rdd):\n",
    "  if not rdd.isEmpty():\n",
    "    spark = SparkSession.builder.appName(\"RDD_to_HDFS\").getOrCreate()\n",
    "\n",
    "    # Converting content of RDD into a list of elements\n",
    "    lst = rdd.collect()\n",
    "    hdfs_path = \"hdfs://localhost:54310/user/ubuntu/dataset\"\n",
    "\n",
    "    # Each row is converted and stored separately due to possible inconsistencies on number of fields\n",
    "    for s in lst:\n",
    "\n",
    "      # Converting to python list because it needs to be iterable\n",
    "      l = [str(s).split(\"/t\")]\n",
    "\n",
    "      # Constructing the schema of DataFrame\n",
    "      schema = \"\"\n",
    "      for i in range(len(l)):\n",
    "        schema = schema+(chr(ord('`')+(i+1)))+\" string, \"\n",
    "      schema = schema[0:len(schema)-2]\n",
    "\n",
    "      # Creating DataFrame and saving it to HDFS\n",
    "      df = spark.createDataFrame(l, schema = schema)\n",
    "      df.write.csv(hdfs_path, mode=\"append\", header=False)"
   ],
   "id": "f0420f0083574327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initiating Spark Streaming\n",
    "\n",
    "In this section we initiate the streaming job after connecting to the streamer service."
   ],
   "id": "6dcf10956fc474fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a local StreamingContext with two working threads and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Separating the lines in the batch and converting\n",
    "batch=lines.map(lambda line: line.split(\"\\n\"))\n",
    "batch.foreachRDD(rdd_to_hdfs)\n",
    "\n",
    "# Start the computation and wait for the termination\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ],
   "id": "29dd037f1bc604c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sc.stop()",
   "id": "7abbaccf68599e81",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
