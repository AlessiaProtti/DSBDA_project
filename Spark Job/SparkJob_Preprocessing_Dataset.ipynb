{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "In this notebook we prepare the dataset by preprocessing it through a SparkJob."
   ],
   "id": "51c6a2763cc4a54e"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:14.350664Z",
     "start_time": "2025-08-31T09:53:14.275590Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_date"
   ],
   "id": "9e4b4260fa244b44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:17.200951Z",
     "start_time": "2025-08-31T09:53:14.369590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sc = pyspark.SparkContext(appName=\"Preprocessing dataset\")\n",
    "spark = SparkSession(sc)"
   ],
   "id": "ecd31d4f6965f423",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/31 11:53:15 WARN Utils: Your hostname, dsbda-vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/08/31 11:53:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/31 11:53:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset\n",
    "\n",
    "In this section we load the dataset inside a Spark DataFrame."
   ],
   "id": "d588b52d08178533"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:20.373165Z",
     "start_time": "2025-08-31T09:53:17.421175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hdfs_path = \"hdfs://localhost:54310/user/ubuntu/dataset/\"\n",
    "schema = \"BookID string, Title string, Authors string, Avg_Rating string, ISBN string, ISBN13 string, Language_Code string, Num_Pages string, Ratings_Count string, Text_Reviews_Count string, Publication_Date string, Publisher string\"\n",
    "\n",
    "df = spark.read.csv(hdfs_path, header=False, inferSchema=False, sep=\",\", quote='', escape='', schema=schema)"
   ],
   "id": "4b2805bf94e7b616",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:20.436526Z",
     "start_time": "2025-08-31T09:53:20.396746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dropping uninformative columns\n",
    "df=df.drop(\"BookID\", \"ISBN\", \"ISBN13\", \"Language_Code\")"
   ],
   "id": "2250ba8b4213326d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "During the injection phase some escaping characters have been added. In particular here we aim to remove the _\\\\_, _[_, _]_ and _\"_ characters.\n",
    "In addition to that, we also removed the _'_ character only at the start and at the end of the row, in order to avoid dropping saxon genitives."
   ],
   "id": "9cdcc1bfe760f571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:20.664932Z",
     "start_time": "2025-08-31T09:53:20.441543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "df = df.select([sf.regexp_replace(c, r'\\\\|\\[|\\]|\\\"', '').alias(c) for c in df.columns])\n",
    "df = df.select([sf.regexp_replace(c, r'^ \\'|\\'$|', '').alias(c) for c in df.columns])"
   ],
   "id": "d6cbf229843fd379",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next step is to cast each column to the appropriate type.",
   "id": "44b260caa135bc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:20.754560Z",
     "start_time": "2025-08-31T09:53:20.670249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Casting column types\n",
    "df = df.withColumn(\"Avg_Rating\", col(\"Avg_Rating\").cast(FloatType())).withColumn(\"Num_Pages\", col(\"Num_Pages\").cast(IntegerType())).withColumn(\"Ratings_Count\", col(\"Ratings_Count\").cast(IntegerType())).withColumn(\"Text_Reviews_Count\", col(\"Text_Reviews_Count\").cast(IntegerType()))\n",
    "\n",
    "# We have different formats for the date, so we unify them\n",
    "df = df.withColumn(\"Pub_Date\",\n",
    "  sf.coalesce(\n",
    "    to_date(col(\"Publication_Date\"), \"M/d/yyyy\"),\n",
    "    to_date(col(\"Publication_Date\"), \"MM/d/yyyy\"),\n",
    "    to_date(col(\"Publication_Date\"), \"M/dd/yyyy\"),\n",
    "    to_date(col(\"Publication_Date\"), \"MM/dd/yyyy\"),\n",
    "  ).alias(\"Pub_Date\"),\n",
    ")\n",
    "\n",
    "df = df.drop(\"Publication_Date\")"
   ],
   "id": "32a604bbbb164b23",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, the preprocessed dataset is saved in Hadoop HDFS.",
   "id": "412fadad3406281f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:48.441165Z",
     "start_time": "2025-08-31T09:53:22.603839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Saving Preprocessed dataset on HDFS\n",
    "hdfs_path_out = \"hdfs://localhost:54310/user/ubuntu/dataset_preprocessed/\"\n",
    "df.write.csv(hdfs_path_out, header=False)"
   ],
   "id": "d30cdebfeb0e4a2a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T09:53:48.626345Z",
     "start_time": "2025-08-31T09:53:48.480058Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "89bf8aa7ffd5613b",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
